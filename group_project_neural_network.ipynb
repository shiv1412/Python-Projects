{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from sklearn.preprocessing import MinMaxScaler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "b'Skipping line 6452: expected 8 fields, saw 9\\nSkipping line 43667: expected 8 fields, saw 10\\nSkipping line 51751: expected 8 fields, saw 9\\n'\n",
      "b'Skipping line 92038: expected 8 fields, saw 9\\nSkipping line 104319: expected 8 fields, saw 9\\nSkipping line 121768: expected 8 fields, saw 9\\n'\n",
      "b'Skipping line 144058: expected 8 fields, saw 9\\nSkipping line 150789: expected 8 fields, saw 9\\nSkipping line 157128: expected 8 fields, saw 9\\nSkipping line 180189: expected 8 fields, saw 9\\nSkipping line 185738: expected 8 fields, saw 9\\n'\n",
      "b'Skipping line 209388: expected 8 fields, saw 9\\nSkipping line 220626: expected 8 fields, saw 9\\nSkipping line 227933: expected 8 fields, saw 11\\nSkipping line 228957: expected 8 fields, saw 10\\nSkipping line 245933: expected 8 fields, saw 9\\nSkipping line 251296: expected 8 fields, saw 9\\nSkipping line 259941: expected 8 fields, saw 9\\nSkipping line 261529: expected 8 fields, saw 9\\n'\n",
      "D:\\Anaconda\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3063: DtypeWarning: Columns (3) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    }
   ],
   "source": [
    "# Reading data files for rating,users and books\n",
    "book_rating_data_set = pd.read_csv('C:/Users/sharm/Downloads/BX-CSV-Dump (1)/BX-Book-Ratings.csv', sep=';', error_bad_lines=False, encoding=\"latin-1\")\n",
    "user_data_set = pd.read_csv('C:/Users/sharm/Downloads/BX-CSV-Dump (1)/BX-Users.csv', sep=';', error_bad_lines=False, encoding=\"latin-1\")\n",
    "book_data_set = pd.read_csv('C:/Users/sharm/Downloads/BX-CSV-Dump (1)/BX-Books.csv', sep=';', error_bad_lines=False, encoding=\"latin-1\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   User-ID        ISBN  Book-Rating            Book-Title\n",
      "0   276725  034545104X            0  Flesh Tones: A Novel\n",
      "1     2313  034545104X            5  Flesh Tones: A Novel\n",
      "2     6543  034545104X            0  Flesh Tones: A Novel\n",
      "3     8680  034545104X            5  Flesh Tones: A Novel\n",
      "4    10314  034545104X            9  Flesh Tones: A Novel\n",
      "5    23768  034545104X            0  Flesh Tones: A Novel\n",
      "6    28266  034545104X            0  Flesh Tones: A Novel\n",
      "7    28523  034545104X            0  Flesh Tones: A Novel\n",
      "8    39002  034545104X            0  Flesh Tones: A Novel\n",
      "9    50403  034545104X            9  Flesh Tones: A Novel\n"
     ]
    }
   ],
   "source": [
    "# Data cleaning \n",
    "# combining ratings file with book file on keeping ISBN and then dropping cols which are not required for further calculation\n",
    "book_rating_file = pd.merge(book_rating_data_set, book_data_set, on='ISBN')\n",
    "cols = ['Year-Of-Publication', 'Publisher', 'Book-Author', 'Image-URL-S', 'Image-URL-M', 'Image-URL-L']\n",
    "book_rating_file.drop(cols, axis=1, inplace=True)\n",
    "print(book_rating_file.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                          Book-Title  RatingCount_book\n",
      "0   A Light in the Storm: The Civil War Diary of ...                 4\n",
      "1                              Always Have Popsicles                 1\n",
      "2               Apple Magic (The Collector's series)                 1\n",
      "3   Ask Lily (Young Women of Faith: Lily Series, ...                 1\n",
      "4   Beyond IBM: Leadership Marketing and Finance ...                 1\n",
      "5   Clifford Visita El Hospital (Clifford El Gran...                 1\n",
      "6                                       Dark Justice                 1\n",
      "7                                           Deceived                 2\n",
      "8   Earth Prayers From around the World: 365 Pray...                10\n",
      "9   Final Fantasy Anthology: Official Strategy Gu...                 4\n"
     ]
    }
   ],
   "source": [
    "# extracting information for finding ratings count for each book based on title and rating by each user\n",
    "rating_counting = (book_rating_file.\n",
    "     groupby(by = ['Book-Title'])['Book-Rating'].\n",
    "     count().\n",
    "     reset_index().\n",
    "     rename(columns = {'Book-Rating': 'RatingCount_book'})\n",
    "     [['Book-Title', 'RatingCount_book']]\n",
    "    )\n",
    "print(rating_counting.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                          Book-Title  RatingCount_book\n",
      "75                      'Salem's Lot                47\n",
      "203                   10 Lb. Penalty                61\n",
      "422                   101 Dalmatians                37\n",
      "673  14,000 Things to Be Happy About                28\n",
      "697               16 Lighthouse Road                65\n",
      "764                             1984               284\n",
      "818              1st to Die: A Novel               509\n",
      "913            2001: A Space Odyssey                25\n",
      "946                2010: Odyssey Two                90\n",
      "955                204 Rosewood Lane                71\n"
     ]
    }
   ],
   "source": [
    "# Finding out the counting of ratings for the threshold value to find out ratings for each book\n",
    "threshold = 25\n",
    "rating_counting = rating_counting.query('RatingCount_book >= @threshold')\n",
    "print(rating_counting.head(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Book-Title  RatingCount_book  User-ID        ISBN  Book-Rating\n",
      "0  'Salem's Lot                47     8936  067103975X            0\n",
      "1  'Salem's Lot                47   172245  067103975X            0\n",
      "2  'Salem's Lot                47   189835  067103975X            5\n",
      "3  'Salem's Lot                47     9226  0451168089            0\n",
      "4  'Salem's Lot                47    33283  0451168089           10\n",
      "5  'Salem's Lot                47    37950  0451168089            0\n",
      "6  'Salem's Lot                47    55734  0451168089            0\n",
      "7  'Salem's Lot                47    56044  0451168089            8\n",
      "8  'Salem's Lot                47    59727  0451168089            0\n",
      "9  'Salem's Lot                47    60263  0451168089           10\n"
     ]
    }
   ],
   "source": [
    "# combining above counted ratings with book rating data set for each book title and extracting the results from rating dataframe\n",
    "user_rating = pd.merge(rating_counting, book_rating_file, left_on='Book-Title', right_on='Book-Title', how='left')\n",
    "print(user_rating.head(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   User-ID  RatingCount_user\n",
      "0        8                 2\n",
      "1        9                 2\n",
      "2       10                 1\n",
      "3       14                 1\n",
      "4       16                 2\n",
      "5       17                 4\n",
      "6       19                 1\n",
      "7       23                 1\n",
      "8       26                 2\n",
      "9       32                 2\n"
     ]
    }
   ],
   "source": [
    "# Finding out user counts for each book rating  for the users\n",
    "user_counting = (user_rating.\n",
    "     groupby(by = ['User-ID'])['Book-Rating'].\n",
    "     count().\n",
    "     reset_index().\n",
    "     rename(columns = {'Book-Rating': 'RatingCount_user'})\n",
    "     [['User-ID', 'RatingCount_user']]\n",
    "    )\n",
    "print(user_counting.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                          Book-Title  RatingCount_book  \\\n",
      "0                                       'Salem's Lot                47   \n",
      "1                                1st to Die: A Novel               509   \n",
      "2                                     A Case of Need               236   \n",
      "3                                 A Perfect Stranger                54   \n",
      "4                                           Accident               126   \n",
      "5                                  All I Need Is You                60   \n",
      "6  All That Remains (Kay Scarpetta Mysteries (Pap...               184   \n",
      "7                                          BODY FARM                50   \n",
      "8                                       Bag of Bones               195   \n",
      "9                                    Best Of Enemies                37   \n",
      "\n",
      "   User-ID        ISBN  Book-Rating  RatingCount_user  \n",
      "0     8936  067103975X            0               177  \n",
      "1     8936  0446610038            0               177  \n",
      "2     8936  0451210638            0               177  \n",
      "3     8936  0440168724            0               177  \n",
      "4     8936  0440217547            0               177  \n",
      "5     8936  0380762609            0               177  \n",
      "6     8936  0380718332            9               177  \n",
      "7     8936  0684195976            0               177  \n",
      "8     8936  067102423X            0               177  \n",
      "9     8936  1551662779            8               177  \n"
     ]
    }
   ],
   "source": [
    "# finding out common results based on user id for each user \n",
    "combined_results = user_rating.merge(user_counting, left_on = 'User-ID', right_on = 'User-ID', how = 'inner')\n",
    "print(combined_results.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# processing and scaling the combined results for the book ratings\n",
    "scaler = MinMaxScaler()\n",
    "combined_results['Book-Rating'] = combined_results['Book-Rating'].values.astype(float)\n",
    "rating_scaled = pd.DataFrame(scaler.fit_transform(combined_results['Book-Rating'].values.reshape(-1,1)))\n",
    "combined_results['Book-Rating'] = rating_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dropping the duplicate results\n",
    "combined_results = combined_results.drop_duplicates(['User-ID', 'Book-Title'])\n",
    "user_book_matrix = combined_results.pivot(index='User-ID', columns='Book-Title', values='Book-Rating')\n",
    "user_book_matrix.fillna(0, inplace=True)\n",
    "users = user_book_matrix.index.tolist()\n",
    "books = user_book_matrix.columns.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From D:\\Anaconda\\lib\\site-packages\\tensorflow_core\\python\\compat\\v2_compat.py:88: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "non-resource variables are not supported in the long term\n"
     ]
    }
   ],
   "source": [
    "# Now implementing tensorflow on the above cleaned combined data results \n",
    "import tensorflow.compat.v1 as tf\n",
    "tf.disable_v2_behavior()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining the parameters for building neural network from scratch\n",
    "num_input = combined_results['Book-Title'].nunique()\n",
    "num_hidden_1 = 10\n",
    "num_hidden_2 = 5\n",
    "\n",
    "X = tf.placeholder(tf.float64, [None, num_input])\n",
    "# calcuating weights and biases for the layers \n",
    "weights = {\n",
    "    'encoder_h1': tf.Variable(tf.random_normal([num_input, num_hidden_1], dtype=tf.float64)),\n",
    "    'encoder_h2': tf.Variable(tf.random_normal([num_hidden_1, num_hidden_2], dtype=tf.float64)),\n",
    "    'decoder_h1': tf.Variable(tf.random_normal([num_hidden_2, num_hidden_1], dtype=tf.float64)),\n",
    "    'decoder_h2': tf.Variable(tf.random_normal([num_hidden_1, num_input], dtype=tf.float64)),\n",
    "}\n",
    "\n",
    "biases = {\n",
    "    'encoder_b1': tf.Variable(tf.random_normal([num_hidden_1], dtype=tf.float64)),\n",
    "    'encoder_b2': tf.Variable(tf.random_normal([num_hidden_2], dtype=tf.float64)),\n",
    "    'decoder_b1': tf.Variable(tf.random_normal([num_hidden_1], dtype=tf.float64)),\n",
    "    'decoder_b2': tf.Variable(tf.random_normal([num_input], dtype=tf.float64)),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining encoders and decoders\n",
    "def encoder(x):\n",
    "    layer_1 = tf.nn.sigmoid(tf.add(tf.matmul(x, weights['encoder_h1']), biases['encoder_b1']))\n",
    "    layer_2 = tf.nn.sigmoid(tf.add(tf.matmul(layer_1, weights['encoder_h2']), biases['encoder_b2']))\n",
    "    return layer_2\n",
    "\n",
    "def decoder(x):\n",
    "    layer_1 = tf.nn.sigmoid(tf.add(tf.matmul(x, weights['decoder_h1']), biases['decoder_b1']))\n",
    "    layer_2 = tf.nn.sigmoid(tf.add(tf.matmul(layer_1, weights['decoder_h2']), biases['decoder_b2']))\n",
    "    return layer_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# invoking encoders and decoders functions\n",
    "encoder_output = encoder(X)\n",
    "decoder_output = decoder(encoder_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predicting valuesbased on the decoder output\n",
    "y_pred = decoder_output\n",
    "y_true = X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From D:\\Anaconda\\lib\\site-packages\\tensorflow_core\\python\\training\\rmsprop.py:119: calling Ones.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "WARNING:tensorflow:From D:\\Anaconda\\lib\\site-packages\\tensorflow_core\\python\\ops\\metrics_impl.py:2029: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Deprecated in favor of operator or tf.math.divide.\n",
      "epoch: 1 Loss: 0.039228938362915265\n",
      "epoch: 2 Loss: 0.00048308506106253596\n",
      "epoch: 3 Loss: 0.0004738396160153606\n",
      "epoch: 4 Loss: 0.00044496947321824657\n",
      "epoch: 5 Loss: 0.00029771864527384763\n",
      "epoch: 6 Loss: 0.00029651056739370975\n",
      "epoch: 7 Loss: 0.00029569363205244405\n",
      "epoch: 8 Loss: 0.00029510227303784206\n",
      "epoch: 9 Loss: 0.00029465452004896016\n",
      "epoch: 10 Loss: 0.0002943039915354508\n",
      "epoch: 11 Loss: 0.0002940223132510237\n",
      "epoch: 12 Loss: 0.000293791152537725\n",
      "epoch: 13 Loss: 0.00029359817474370155\n",
      "epoch: 14 Loss: 0.0002934347379617547\n",
      "epoch: 15 Loss: 0.000293294551096658\n",
      "epoch: 16 Loss: 0.0002931731208834618\n",
      "epoch: 17 Loss: 0.00029306689731467984\n",
      "epoch: 18 Loss: 0.0002929732449644564\n",
      "epoch: 19 Loss: 0.0002928901389230875\n",
      "epoch: 20 Loss: 0.0002928159677933353\n",
      "epoch: 21 Loss: 0.00029274929174700413\n",
      "epoch: 22 Loss: 0.0002926890809373849\n",
      "epoch: 23 Loss: 0.00029263445252153734\n",
      "epoch: 24 Loss: 0.0002925846598934774\n",
      "epoch: 25 Loss: 0.0002925391288276753\n",
      "epoch: 26 Loss: 0.00029249738747917127\n",
      "epoch: 27 Loss: 0.00029245894039181\n",
      "epoch: 28 Loss: 0.00029242339361171976\n",
      "epoch: 29 Loss: 0.00029239043587771376\n",
      "epoch: 30 Loss: 0.0002923598190760708\n",
      "epoch: 31 Loss: 0.00029233132697442546\n",
      "epoch: 32 Loss: 0.00029230474433015666\n",
      "epoch: 33 Loss: 0.0002922798630785181\n",
      "epoch: 34 Loss: 0.00029225653244401316\n",
      "epoch: 35 Loss: 0.0002922346145133619\n",
      "epoch: 36 Loss: 0.00029221399463673223\n",
      "epoch: 37 Loss: 0.00029219458150798935\n",
      "epoch: 38 Loss: 0.00029217627792711617\n",
      "epoch: 39 Loss: 0.00029215901833916125\n",
      "epoch: 40 Loss: 0.0002921426917590211\n",
      "epoch: 41 Loss: 0.0002921271911873191\n",
      "epoch: 42 Loss: 0.0002921124770967022\n",
      "epoch: 43 Loss: 0.000292098482469894\n",
      "epoch: 44 Loss: 0.00029208515643760764\n",
      "epoch: 45 Loss: 0.0002920724752861062\n",
      "epoch: 46 Loss: 0.0002920603735763543\n",
      "epoch: 47 Loss: 0.0002920488031140492\n",
      "epoch: 48 Loss: 0.0002920377563646516\n",
      "epoch: 49 Loss: 0.00029202720250788274\n",
      "epoch: 50 Loss: 0.00029201710155718377\n",
      "epoch: 51 Loss: 0.0002920074304720223\n",
      "epoch: 52 Loss: 0.0002919981734254072\n",
      "epoch: 53 Loss: 0.00029198930350699524\n",
      "epoch: 54 Loss: 0.0002919807754159253\n",
      "epoch: 55 Loss: 0.0002919725751932371\n",
      "epoch: 56 Loss: 0.0002919646808906834\n",
      "epoch: 57 Loss: 0.00029195709217389116\n",
      "epoch: 58 Loss: 0.0002919497921280423\n",
      "epoch: 59 Loss: 0.0002919427385730909\n",
      "epoch: 60 Loss: 0.0002919359410631895\n",
      "epoch: 61 Loss: 0.0002919293808734473\n",
      "epoch: 62 Loss: 0.0002919230450123566\n",
      "epoch: 63 Loss: 0.0002919169371936875\n",
      "epoch: 64 Loss: 0.00029191104343618803\n",
      "epoch: 65 Loss: 0.00029190532419913023\n",
      "epoch: 66 Loss: 0.0002918997968743711\n",
      "epoch: 67 Loss: 0.00029189444843919486\n",
      "epoch: 68 Loss: 0.000291889263017569\n",
      "epoch: 69 Loss: 0.0002918842367664326\n",
      "epoch: 70 Loss: 0.00029187937360017943\n",
      "epoch: 71 Loss: 0.00029187467113807347\n",
      "epoch: 72 Loss: 0.00029187010056161585\n",
      "epoch: 73 Loss: 0.00029186566213830505\n",
      "epoch: 74 Loss: 0.000291861357419632\n",
      "epoch: 75 Loss: 0.00029185718158616653\n",
      "epoch: 76 Loss: 0.00029185313215017303\n",
      "epoch: 77 Loss: 0.0002918491939801564\n",
      "epoch: 78 Loss: 0.00029184537739709805\n",
      "epoch: 79 Loss: 0.00029184167208447486\n",
      "epoch: 80 Loss: 0.00029183807355722964\n",
      "epoch: 81 Loss: 0.0002918345727382485\n",
      "epoch: 82 Loss: 0.0002918311633814429\n",
      "epoch: 83 Loss: 0.000291827837452943\n",
      "epoch: 84 Loss: 0.00029182458407893706\n",
      "epoch: 85 Loss: 0.0002918214218862333\n",
      "epoch: 86 Loss: 0.0002918183422747568\n",
      "epoch: 87 Loss: 0.00029181533326057735\n",
      "epoch: 88 Loss: 0.00029181239910137854\n",
      "epoch: 89 Loss: 0.00029180954156710835\n",
      "epoch: 90 Loss: 0.000291806752030942\n",
      "epoch: 91 Loss: 0.00029180402709119105\n",
      "epoch: 92 Loss: 0.0002918013692266743\n",
      "epoch: 93 Loss: 0.0002917987739166682\n",
      "epoch: 94 Loss: 0.0002917962348080847\n",
      "epoch: 95 Loss: 0.00029179375075513876\n",
      "epoch: 96 Loss: 0.00029179131787018646\n",
      "epoch: 97 Loss: 0.00029178893210508465\n",
      "epoch: 98 Loss: 0.0002917866081767059\n",
      "epoch: 99 Loss: 0.00029178433633919045\n",
      "epoch: 100 Loss: 0.00029178211082794673\n"
     ]
    }
   ],
   "source": [
    "# calculating loss values and optimizing the results  and then predicting the data\n",
    "loss = tf.losses.mean_squared_error(y_true, y_pred)\n",
    "optimizer = tf.train.RMSPropOptimizer(0.03).minimize(loss)\n",
    "eval_x = tf.placeholder(tf.int32, )\n",
    "eval_y = tf.placeholder(tf.int32, )\n",
    "pre, pre_op = tf.metrics.precision(labels=eval_x, predictions=eval_y)\n",
    "init = tf.global_variables_initializer()\n",
    "local_init = tf.local_variables_initializer()\n",
    "predicted_data = pd.DataFrame()\n",
    "#iterating through data for calculating the predicting values for each epoch\n",
    "with tf.Session() as session:\n",
    "    epochs = 100\n",
    "    batch_size = 35\n",
    "    session.run(init)\n",
    "    session.run(local_init)\n",
    "    num_batches = int(user_book_matrix.shape[0] / batch_size)\n",
    "    user_book_matrix = np.array_split(user_book_matrix, num_batches)\n",
    "    \n",
    "    for i in range(epochs):\n",
    "        avg_cost = 0\n",
    "        for batch in user_book_matrix:\n",
    "            _, l = session.run([optimizer, loss], feed_dict={X: batch})\n",
    "            avg_cost += l\n",
    "\n",
    "        avg_cost /= num_batches\n",
    "\n",
    "        print(\"epoch: {} Loss: {}\".format(i + 1, avg_cost))\n",
    "    # doing calculations on predicted data for users book rating \n",
    "    user_book_matrix = np.concatenate(user_book_matrix, axis=0)\n",
    "    predictions = session.run(decoder_output, feed_dict={X: user_book_matrix})\n",
    "    predicted_data = predicted_data.append(pd.DataFrame(predictions))\n",
    "    predicted_data = predicted_data.stack().reset_index(name='Book-Rating')\n",
    "    predicted_data.columns = ['User-ID', 'Book-Title', 'Book-Rating']\n",
    "    predicted_data['User-ID'] = predicted_data['User-ID'].map(lambda value: users[value])\n",
    "    predicted_data['Book-Title'] = predicted_data['Book-Title'].map(lambda value: books[value])\n",
    "    keys = ['User-ID', 'Book-Title']\n",
    "    index_first_column = predicted_data.set_index(keys).index\n",
    "    index_second_column = combined_results.set_index(keys).index\n",
    "    # finding out the top ranked results from the predicted values\n",
    "    top_ranked = predicted_data[~index_first_column.isin(index_second_column)]\n",
    "    top_ranked = top_ranked.sort_values(['User-ID', 'Book-Rating'], ascending=[True, False])\n",
    "    top_ranked = top_ranked.groupby('User-ID').head(10)\n",
    "    print(top_ranked)\n",
    "    \n",
    "    # testing results for one of the record\n",
    "    print(top_ranked.loc[top_ranked['User-ID'] == 180187])\n",
    "    print(book_rating_data_set.loc[book_rating_data_set['User-ID'] == 180187].sort_values(by=['Book-Rating'], ascending=False))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
